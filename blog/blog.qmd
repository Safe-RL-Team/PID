---
title: "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble"
description: "based on [@an2021edac]"
author:
    - name: Julian Dralle
      affiliation: TU Berlin
    - name: Jonas Loos
      affiliation: TU Berlin
bibliography: references.bib
abstract: |
    TODO: abstract
---

# Introduction

TODO

# Preliminaries

TODO

# Soft Actor-Critic (SAC-N)

The paper introduces two new methods for offline RL. The first method is called SAC-N and is an extension of Soft Actor-Critic (SAC) [@Haarnoja2018SoftAO], which is a popular off-policy actor-critic deep RL algorithm.
SAC-N extends SAC by using the minimum q-value of N instead of two q-functions, i.e. critics. The idea behind using the minimum of more critics is that the resulting q-value is more pessimistic and therefore more likely to avoid erroneously high q-values. This is especially important for offline RL, where the q-values are estimated from a limited dataset and erroneouly high q-values cannot be corrected through exploration.

The minimum of multiple critics approximates the true q-value minus a multiple of the standard deviation [@an2021edac]:

$$ \mathbb{E}\left [\min_{i=1,...,N}Q_i\right] \approx m - \Phi^{-1}\left(\frac{N-\pi/8}{N-\pi/4+1}\right) \sigma $$

Where $N$ is the number of critics, $Q_i$ is the q-value of the $i$-th critic, $m$ is the true q-value, $\Phi$ is the CDF of the standard gaussian distribution, and $\sigma$ is the standard deviation.

This is visualized in the diagram below, where q-value estimates over an exemplrary action space are plotted.The black line is the true q-value and the grey area its standard deviation. The lightblue lines represent the critics, that try to approximate the true q-value. The bottom blue line is the minimum of the critics, that should, especially for a high number of critics, be roughly the true q-value minus a multiple of the standard deviation. You can use the slider to change the number of critics:

```{ojs}
//| echo: false
means = [10,12, 9,10,11,11,12,10, 9, 8,10,11,12,14,13,13,11,10, 9,10];
stds  = [ 2, 3, 2, 1, 2, 3, 2, 3, 4, 2, 3, 4, 2, 1, 3, 2, 1, 2, 3, 2];
viewof num_critics = Inputs.range([1,50], {value:20, step:1, label: "number of critics"});
function gaussianRandom(mean=0, stdev=1) {
    let u = 1 - Math.random(); // Converting [0,1) to (0,1]
    let v = Math.random();
    let z = Math.sqrt(-2.0*Math.log(u)) * Math.cos(2.0*Math.PI*v);
    return z * stdev + mean;
}
function example_critic() {
  return [...Array(means.length).keys()].map((i)=>gaussianRandom(means[i], stds[i]));
}
function toPlot(data, color){
  return Plot.line(data.map((x, i)=>({"action space": i, "q-value": x})), {x: "action space", y: "q-value", stroke: color});
}
critics = [...Array(num_critics).keys()].map(i=>example_critic());
Plot.plot({
  marks: [
      Plot.areaY(means.map((x,i)=>({"action space": i, low:x-stds[i], high:x+stds[i]})), {x: "action space", y1: "low", y2: "high", fill: "#ddd"}),
      ...(critics.map(x=>toPlot(x, "lightblue"))),
      toPlot([...Array(means.length).keys()].map(i=>Math.min(...critics.map(x=>x[i]))), "blue"),
      toPlot(means, "black"),
  ],
  y: {
    domain: [0, 20],
    label: "q-value"
  }
});
```

SAC-N already achieve notable performance and beats the previous state of the art, CQL, as will be shown in [Results](#results). However, SAC-N requires a large number of critics, which comes with a high computational cost. Therefore, the paper introduces a second method, EDAC, that is more efficient.


# Ensemble-Diversified Actor Critic (EDAC)

EDAC improves over SAC-N, by introducing an ensemble gradient diversification term to the optimization of the ensemble of critics:

$$ \underset\phi{\text{minimize}}\ \ \frac{1}{N-1} \sum_{1\leq i\neq j \leq N} \langle \nabla_a Q_{\phi_i}, \nabla_a Q_{\phi_j} \rangle $$


# Results
\label{sec-results}



# Conclusion

TODO


---

# Example Plot

```{python}
#| title: Example Plot
#| label: fig-example-plot
#| fig-cap: An example plot that shows a sine wave.
#| code-fold: true
import matplotlib.pyplot as plt
import numpy as np

# make an example plot
x = np.linspace(0, 2*np.pi, 100)
y = np.sin(x)

plt.plot(x, y)
```

This section is just exemplary. @fig-example-plot shows an example plot.

# Filler Content

> ChatGPT: write a blog article in markdown about "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble"

## Introduction

Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems in a wide range of fields, including robotics, finance, and game AI. However, one major challenge in RL is how to effectively learn from limited data. Offline RL addresses this challenge by allowing RL agents to learn from previously collected data, rather than through trial-and-error interactions with the environment.

In this article, we will discuss a recent approach to offline RL called "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble" (UOFRL-DQE). This approach addresses the problem of limited data by leveraging uncertainty estimates to guide the learning process and by creating a diverse ensemble of Q-functions to improve the robustness of the learned policy.

## Background

In RL, an agent interacts with an environment and learns to take actions that maximize a cumulative reward signal. The agent's decision-making process is guided by a Q-function, which estimates the expected reward for taking a given action in a given state. In offline RL, the agent is trained on a dataset of transitions (state, action, reward, next state) collected from past interactions with the environment.

One major challenge in offline RL is the problem of distributional shift, which occurs when the distribution of states and actions encountered during training is different from the distribution encountered during deployment. This can lead to poor performance of the learned policy.

## UOFRL-DQE Approach

UOFRL-DQE addresses the problem of distributional shift by creating a diverse ensemble of Q-functions. Each Q-function in the ensemble is trained on a different subset of the training data, and the ensemble is used to estimate the Q-value of a given state-action pair.

In addition to creating a diverse ensemble, UOFRL-DQE also leverages uncertainty estimates to guide the learning process. The approach estimates the uncertainty of the Q-value estimates by measuring the variance of the Q-values across the ensemble. The agent then prioritizes learning from transitions with high uncertainty, in order to improve the overall performance of the learned policy.

## Evaluation

The effectiveness of UOFRL-DQE has been evaluated in a variety of tasks, including robotic manipulation and game playing. The results have shown that UOFRL-DQE is able to effectively leverage uncertainty estimates and the diversity of the ensemble to improve the robustness of the learned policy, leading to better performance in comparison to other offline RL methods.

## Conclusion

Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble (UOFRL-DQE) is a powerful approach to offline RL that addresses the problem of limited data by leveraging uncertainty estimates and creating a diverse ensemble of Q-functions. The approach has been shown to be effective in a variety of tasks and has the potential to significantly improve the performance of RL agents in real-world applications.
