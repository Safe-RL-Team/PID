{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2ce5283f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\"\n",
    "description: \"based on [@an2021edac]\"\n",
    "author:\n",
    "    - name: Julian Dralle\n",
    "      affiliation: TU Berlin\n",
    "    - name: Jonas Loos\n",
    "      affiliation: TU Berlin\n",
    "bibliography: references.bib\n",
    "abstract: |\n",
    "    TODO: abstract\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612dbb90",
   "metadata": {},
   "source": [
    "# Example Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig-example-plot",
   "metadata": {
    "title": "Example Plot"
   },
   "outputs": [],
   "source": [
    "#| label: fig-example-plot\n",
    "#| fig-cap: An example plot that shows a sine wave.\n",
    "#| code-fold: true\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# make an example plot\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f2ad2-0d2e-4e23-97d7-1874247f2886",
   "metadata": {},
   "source": [
    "# Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Why Offline-RL?\n",
    "\n",
    "Training of RL algorithms require **active interaction** with the environment. Training can become quite time-consuming and expensive. It can even be dangerous in safety-critical domains like driving or healthcare. A trial-and-error procedure is basically prohibited. We cannot as an example let an agent explore, make mistakes, and learn while treating patients in a hospital.\n",
    "That's what makes learning from pre-collected experience so relevant. And fortunately we have already in many domains existing large datasets.\n",
    "Offline-RL therefore aims to learn policies using only these pre-collected data without further interactions with the environment.\n",
    "\n",
    "![Online and Offline Reinforcement Learning](https://1.bp.blogspot.com/-O0FvK3zJd9w/XpXqiJduwyI/AAAAAAAAFtM/5hxzdWOoSLw5sd5vEgMsiGVJSATKx1oEgCLcBGAsYHQ/s640/OFFLINE%2BRL%2Bfig1%2B05b.gif)\n",
    "<!--- adjust size and center -->\n",
    "\n",
    "### What properties make offline-RL difficult?\n",
    "\n",
    "But offline RL comes with its own challenges. By far the biggest problem are so called **out of distribution (OOD) actions**. OOD actions refer to actions taken by an agent that fall outside the range of actions observed in the training dataset. \n",
    "State-action space can become so vast that the dataset cannot cover all of it. Especially narrow and biased datasets lack significant coverage and can lead to problems with OOD actions. For example, healthcare datasets are often biased towards serious cases. Only seriously ill people are getting treated, while healthier people are sent home untreated.\n",
    "![Exampe OOD actions](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1591922248906_d4rl_medicine.png)\n",
    "<!--- adjust size -->\n",
    "A naive algorithm might now conclude that treatment causes death, since there were no fatalities in the untreated (= healthy) patients. Choosing to not treat a severely sick patient is something that never happened in the data, since the doctor would thereby violate his duty of care. Not treating a sick patient is therefore an OOD action. \n",
    "Vanilla RL algorithm might heavily overestimate the Q-values of OOD state-action pairs.\n",
    "\n",
    "### How to deal with OOD state-actions?\n",
    "\n",
    "\"Avoid OOD state-actions!\", has been the approach of many offline RL algorithms. This can be achieved by regularizing the policy to be close to the behavior policy that was used to collect the data. A more recent approach is to penalize the Q-values to be more pessimistic as done in Conservative Q-learning for Offline RL (CQL).\n",
    "But if we use this approach we require either (a) an estimation of the behavior policy or (b) explicit sampling from OOD data points (difficult!). Further, we prohibit our agent to approach any OOD state-actions, while some of these might actually be good. Q-function networks **do** have the ability to generalize. It's all about **handling the uncertainty** of these predictions. The agent might benefit from choosing some OOD data points which Q-values we can predict with high confidence.\n",
    "With SAC-N and EDAC An et al. (2021) found a way of effectively quantifying the Q-value estimates by an ensemble of Q-function networks. In this blog we will explore and explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a886c5a",
   "metadata": {},
   "source": [
    "This section is just exemplary. @fig-example-plot shows an example plot.\n",
    "\n",
    "# Filler Content\n",
    "\n",
    "> ChatGPT: write a blog article in markdown about \"Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\"\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems in a wide range of fields, including robotics, finance, and game AI. However, one major challenge in RL is how to effectively learn from limited data. Offline RL addresses this challenge by allowing RL agents to learn from previously collected data, rather than through trial-and-error interactions with the environment.\n",
    "\n",
    "In this article, we will discuss a recent approach to offline RL called \"Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\" (UOFRL-DQE). This approach addresses the problem of limited data by leveraging uncertainty estimates to guide the learning process and by creating a diverse ensemble of Q-functions to improve the robustness of the learned policy.\n",
    "\n",
    "## Background\n",
    "\n",
    "In RL, an agent interacts with an environment and learns to take actions that maximize a cumulative reward signal. The agent's decision-making process is guided by a Q-function, which estimates the expected reward for taking a given action in a given state. In offline RL, the agent is trained on a dataset of transitions (state, action, reward, next state) collected from past interactions with the environment.\n",
    "\n",
    "One major challenge in offline RL is the problem of distributional shift, which occurs when the distribution of states and actions encountered during training is different from the distribution encountered during deployment. This can lead to poor performance of the learned policy.\n",
    "\n",
    "## UOFRL-DQE Approach\n",
    "\n",
    "UOFRL-DQE addresses the problem of distributional shift by creating a diverse ensemble of Q-functions. Each Q-function in the ensemble is trained on a different subset of the training data, and the ensemble is used to estimate the Q-value of a given state-action pair.\n",
    "\n",
    "In addition to creating a diverse ensemble, UOFRL-DQE also leverages uncertainty estimates to guide the learning process. The approach estimates the uncertainty of the Q-value estimates by measuring the variance of the Q-values across the ensemble. The agent then prioritizes learning from transitions with high uncertainty, in order to improve the overall performance of the learned policy.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The effectiveness of UOFRL-DQE has been evaluated in a variety of tasks, including robotic manipulation and game playing. The results have shown that UOFRL-DQE is able to effectively leverage uncertainty estimates and the diversity of the ensemble to improve the robustness of the learned policy, leading to better performance in comparison to other offline RL methods.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble (UOFRL-DQE) is a powerful approach to offline RL that addresses the problem of limited data by leveraging uncertainty estimates and creating a diverse ensemble of Q-functions. The approach has been shown to be effective in a variety of tasks and has the potential to significantly improve the performance of RL agents in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
