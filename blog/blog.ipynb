{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2ce5283f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\"\n",
    "description: \"based on [@an2021edac]\"\n",
    "author:\n",
    "    - name: Julian Dralle\n",
    "      affiliation: TU Berlin\n",
    "    - name: Jonas Loos\n",
    "      affiliation: TU Berlin\n",
    "bibliography: references.bib\n",
    "abstract: |\n",
    "    TODO: abstract\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f2ad2-0d2e-4e23-97d7-1874247f2886",
   "metadata": {},
   "source": [
    "# Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Why Offline-RL?\n",
    "\n",
    "Training of RL algorithms require **active interaction** with the environment. Training can become quite time-consuming and expensive. It can even be dangerous in safety-critical domains like driving or healthcare. A trial-and-error procedure is basically prohibited. We cannot as an example let an agent explore, make mistakes, and learn while treating patients in a hospital.\n",
    "That's what makes learning from pre-collected experience so relevant. And fortunately we have already in many domains existing large datasets.\n",
    "Offline-RL therefore aims to learn policies using only these pre-collected data without further interactions with the environment.\n",
    "\n",
    "![Online and Offline Reinforcement Learning](https://1.bp.blogspot.com/-O0FvK3zJd9w/XpXqiJduwyI/AAAAAAAAFtM/5hxzdWOoSLw5sd5vEgMsiGVJSATKx1oEgCLcBGAsYHQ/s640/OFFLINE%2BRL%2Bfig1%2B05b.gif)\n",
    "<!--- adjust size and center -->\n",
    "\n",
    "### What properties make offline-RL difficult?\n",
    "\n",
    "But offline RL comes with its own challenges. By far the biggest problem are so called **out of distribution (OOD) actions**. OOD actions refer to actions taken by an agent that fall outside the range of actions observed in the training dataset. \n",
    "State-action space can become so vast that the dataset cannot cover all of it. Especially narrow and biased datasets lack significant coverage and can lead to problems with OOD actions. OOD state actions might have erroneously high Q-values and since we cannot get any feedback from the environment in the offline setting, this would lead to error propagation. The error is repeated over and over again.\n",
    "\n",
    "For example, healthcare datasets are often biased towards serious cases. Only seriously ill people are getting treated, while healthier people are sent home untreated.\n",
    "![Exampe OOD actions](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1591922248906_d4rl_medicine.png)\n",
    "<!--- adjust size -->\n",
    "A naive algorithm might now conclude that treatment causes death, since there were no fatalities in the untreated (= healthy) patients. Choosing to not treat a severely sick patient is something that never happened in the data, since the doctor would thereby violate his duty of care. Not treating a sick patient is therefore an OOD action. \n",
    "Vanilla RL algorithm might heavily overestimate the Q-values of OOD state-action pairs.\n",
    "\n",
    "### How to deal with OOD state-actions?\n",
    "\n",
    "\"Avoid OOD state-actions!\", has been the approach of many offline RL algorithms. This can be achieved by regularizing the policy to be close to the behavior policy that was used to collect the data. A more recent approach is to penalize the Q-values to be more pessimistic as done in Conservative Q-learning for Offline RL (CQL).\n",
    "But if we use this approach we require either (a) an estimation of the behavior policy or (b) explicit sampling from OOD data points (difficult!). Further, we prohibit our agent to approach any OOD state-actions, while some of these might actually be good. Q-function networks **do** have the ability to generalize. It's all about **handling the uncertainty** of these predictions. The agent might benefit from choosing some OOD data points which Q-values we can predict with high confidence.\n",
    "With SAC-N and EDAC An et al. (2021) found a way of **effectively quantifying the Q-value estimates** by an ensemble of Q-function networks. In this blog we will explore and explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d15ad6-be20-4cdc-bea7-c22023923d4c",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "\n",
    "### Q-Learning\n",
    "Like in standard reinforcement learning we want to find a policy $\\pi(a | s)$ that maximizes the cumulative discounted reward $\\mathbb{E}_{s_t, a_t}[...]$. \n",
    "The model-free [Q-learning](https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c) algorithm is a very common approach to learn the Q-function $Q_{\\phi}(s,a)$ with-in a neural network.\n",
    "\n",
    "### Actor-critic method\n",
    "In the standard deep actor-critic approach we use two networks: (1) a policy-based actor network and (2) a value-based critic network.\n",
    "\n",
    "![Strucutre of deep actor-critic RL](https://www.mdpi.com/applsci/applsci-10-04236/article_deploy/html/images/applsci-10-04236-g005.png)\n",
    "The critic network minimizes the Bellman residual. Note: In offline RL transitions are sampled from a static dataset $D$\n",
    "\n",
    "![text](./pictures/critic_formula.png)\n",
    "\n",
    "The actor network is updated in an alternating fashion to maximizes the expected Q-value.\n",
    "\n",
    "![text](./pictures/actor_formula.png)\n",
    "\n",
    "### Conservative Q-Learning\n",
    "\n",
    "As of 2021, [Conservative Q-Learning](https://arxiv.org/abs/2006.04779) (CQL) is the state-of-the-art for offline RL. It uses a “simple Q-value regularizer” to prevent the overestimation of OOD actions. \n",
    "\n",
    "![text](./pictures/cql.png)\n",
    "\n",
    "For each state, CQL computes a distribution over actions using a temperature parameter $\\alpha$ that controls the amount of exploration. The distribution is a mixture of the behavior policy and the current Q-function. The closer $\\alpha$ is to 1 the more conservative.\n",
    "\n",
    "CQL will be used as the baseline.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c651ef-5fc8-4d69-83f4-3d9edb5f0a6b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3b9aa",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic (SAC-N)\n",
    "\n",
    "The paper introduces two new methods for offline RL. The first method is called SAC-N and is an extension of Soft Actor-Critic (SAC) [@Haarnoja2018SoftAO], which is a popular off-policy actor-critic deep RL algorithm.\n",
    "SAC-N extends SAC by using the minimum q-value of N instead of two q-functions, i.e. critics. The idea behind using the minimum of more critics is that the resulting q-value is more pessimistic and therefore more likely to avoid erroneously high q-values. This is especially important for offline RL, where the q-values are estimated from a limited dataset and erroneouly high q-values cannot be corrected through exploration.\n",
    "\n",
    "The minimum of multiple critics approximates the true q-value minus a multiple of the standard deviation [@an2021edac]:\n",
    "\n",
    "$$ \\mathbb{E}\\left [\\min_{i=1,...,N}Q_i\\right] \\approx m - \\Phi^{-1}\\left(\\frac{N-\\pi/8}{N-\\pi/4+1}\\right) \\sigma $$\n",
    "\n",
    "Where $N$ is the number of critics, $Q_i$ is the q-value of the $i$-th critic, $m$ is the true q-value, $\\Phi$ is the CDF of the standard gaussian distribution, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "This is visualized in the diagram below, where q-value estimates over an exemplrary action space are plotted.The black line is the true q-value and the grey area its standard deviation. The lightblue lines represent the critics, that try to approximate the true q-value. The bottom blue line is the minimum of the critics, that should, especially for a high number of critics, be roughly the true q-value minus a multiple of the standard deviation. You can use the slider to change the number of critics:\n",
    "\n",
    "```{ojs}\n",
    "//| echo: false\n",
    "means = [10,12, 9,10,11,11,12,10, 9, 8,10,11,12,14,13,13,11,10, 9,10];\n",
    "stds  = [ 2, 3, 2, 1, 2, 3, 2, 3, 4, 2, 3, 4, 2, 1, 3, 2, 1, 2, 3, 2];\n",
    "viewof num_critics = Inputs.range([1,50], {value:20, step:1, label: \"number of critics\"});\n",
    "function gaussianRandom(mean=0, stdev=1) {\n",
    "    let u = 1 - Math.random(); // Converting [0,1) to (0,1]\n",
    "    let v = Math.random();\n",
    "    let z = Math.sqrt(-2.0*Math.log(u)) * Math.cos(2.0*Math.PI*v);\n",
    "    return z * stdev + mean;\n",
    "}\n",
    "function example_critic() {\n",
    "  return [...Array(means.length).keys()].map((i)=>gaussianRandom(means[i], stds[i]));\n",
    "}\n",
    "function toPlot(data, color){\n",
    "  return Plot.line(data.map((x, i)=>({\"action space\": i, \"q-value\": x})), {x: \"action space\", y: \"q-value\", stroke: color});\n",
    "}\n",
    "critics = [...Array(num_critics).keys()].map(i=>example_critic());\n",
    "Plot.plot({\n",
    "  marks: [\n",
    "      Plot.areaY(means.map((x,i)=>({\"action space\": i, low:x-stds[i], high:x+stds[i]})), {x: \"action space\", y1: \"low\", y2: \"high\", fill: \"#ddd\"}),\n",
    "      ...(critics.map(x=>toPlot(x, \"lightblue\"))),\n",
    "      toPlot([...Array(means.length).keys()].map(i=>Math.min(...critics.map(x=>x[i]))), \"blue\"),\n",
    "      toPlot(means, \"black\"),\n",
    "  ],\n",
    "  y: {\n",
    "    domain: [0, 20],\n",
    "    label: \"q-value\"\n",
    "  }\n",
    "});\n",
    "```\n",
    "\n",
    "SAC-N already achieve notable performance and beats the previous state of the art, CQL, as will be shown in [Results](#results). However, SAC-N requires a large number of critics, which comes with a high computational cost. Therefore, the paper introduces a second method, EDAC, that is more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682613d3",
   "metadata": {},
   "source": [
    "## Ensemble-Diversified Actor Critic (EDAC)\n",
    "\n",
    "EDAC improves over SAC-N, by introducing an ensemble gradient diversification term to the optimization of the ensemble of critics:\n",
    "\n",
    "$$ \\underset\\phi{\\text{minimize}}\\ \\ \\frac{1}{N-1} \\sum_{1\\leq i\\neq j \\leq N} \\langle \\nabla_a Q_{\\phi_i}, \\nabla_a Q_{\\phi_j} \\rangle $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3bee1",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
