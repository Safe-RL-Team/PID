{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2ce5283f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\"\n",
    "description: \"based on [@an2021edac]\"\n",
    "date: today  # TODO: change to date of publication\n",
    "author:\n",
    "    - name: Julian Dralle\n",
    "      affiliation: TU Berlin\n",
    "    - name: Jonas Loos\n",
    "      affiliation: TU Berlin\n",
    "bibliography: references.bib\n",
    "abstract: |\n",
    "    TODO: abstract\n",
    "jupyter: python3\n",
    "execute:\n",
    "  enabled: true\n",
    "  echo: false\n",
    "  output: true\n",
    "theme:\n",
    "  light: \n",
    "    - default\n",
    "    - custom.scss\n",
    "  dark:\n",
    "    - darkly\n",
    "    - custom.scss\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f2ad2-0d2e-4e23-97d7-1874247f2886",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Why Offline-RL?\n",
    "\n",
    "Training of RL algorithms require **active interaction** with the environment. Training can become quite time-consuming and expensive. It can even be dangerous in safety-critical domains like driving or healthcare. A trial-and-error procedure is basically prohibited. We cannot as an example let an agent explore, make mistakes, and learn while treating patients in a hospital.\n",
    "That's what makes learning from pre-collected experience so relevant. And fortunately we have already in many domains existing large datasets.\n",
    "Offline-RL therefore aims to learn policies using only these pre-collected data without further interactions with the environment.\n",
    "\n",
    "![Online and Offline Reinforcement Learning^[Figure taken from TODO]](figures/offline_rl.gif){#fig-orl fig-align=\"center\" width=80%}\n",
    "\n",
    "### What properties make offline-RL difficult?\n",
    "\n",
    "But offline RL comes with its own challenges. By far the biggest problem are so called **out of distribution (OOD) actions**. OOD actions refer to actions taken by an agent that fall outside the range of actions observed in the training dataset. \n",
    "State-action space can become so vast that the dataset cannot cover all of it. Especially narrow and biased datasets lack significant coverage and can lead to problems with OOD actions. For example, healthcare datasets are often biased towards serious cases. Only seriously ill people are getting treated, while healthier people are sent home untreated.\n",
    "\n",
    "![Example Out of Distribution Actions^[Figure taken from @d4rl_post]](figures/ood_medicine.png){#fig-ood-medicine fig-align=\"center\" width=80%}\n",
    "\n",
    "A naive algorithm might now conclude that treatment causes death, since there were no fatalities in the untreated (= healthy) patients. Choosing to not treat a severely sick patient is something that never happened in the data, since the doctor would thereby violate his duty of care. Not treating a sick patient is therefore an OOD action. \n",
    "Vanilla RL algorithm might heavily overestimate the Q-values of OOD state-action pairs.\n",
    "\n",
    "### How to deal with OOD state-actions?\n",
    "\n",
    "\"Avoid OOD state-actions!\", has been the approach of many offline RL algorithms. This can be achieved by regularizing the policy to be close to the behavior policy that was used to collect the data. A more recent approach is to penalize the Q-values to be more pessimistic as done in Conservative Q-learning for Offline RL (CQL).\n",
    "But if we use this approach we require either (a) an estimation of the behavior policy or (b) explicit sampling from OOD data points (difficult!). Further, we prohibit our agent to approach any OOD state-actions, while some of these might actually be good. Q-function networks **do** have the ability to generalize. It's all about **handling the uncertainty** of these predictions. The agent might benefit from choosing some OOD data points which Q-values we can predict with high confidence.\n",
    "With SAC-N and EDAC An et al. (2021) found a way of **effectively quantifying the Q-value estimates** by an ensemble of Q-function networks. In this blog we will explore and explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d15ad6-be20-4cdc-bea7-c22023923d4c",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "\n",
    "### Q-Learning\n",
    "Like in standard reinforcement learning we want to find a policy $\\pi(a | s)$ that maximizes the cumulative discounted reward $\\mathbb{E}_{s_t, a_t}[...]$. \n",
    "The model-free [Q-learning](https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c) algorithm is a very common approach to learn the Q-function $Q_{\\phi}(s,a)$ with-in a neural network.\n",
    "\n",
    "### Actor-critic method\n",
    "In the standard deep actor-critic approach we use two networks: (1) a policy-based actor network and (2) a value-based critic network.\n",
    "\n",
    "![Structure of deep actor-critic RL^[Figure taken from @app10124236]](figures/actor_critic.webp){fig-align=\"center\" #fig-actor-critic width=80%}\n",
    "\n",
    "The critic network minimizes the Bellman residual. Note: In offline RL transitions are sampled from a static dataset $D$\n",
    "\n",
    "<!-- ![](figures/critic_formula.png) -->\n",
    "\n",
    "$$J_q(Q_\\phi) := \\mathbb{E}_{(s,a,s') \\sim D} \\left[ \\left( Q_\\phi(s,a) - \\left ( r(s,a) + \\gamma\\ \\mathbb{E}_{a'\\sim\\pi_\\phi(\\cdot|s')}[Q_{\\phi'}(s',a')] \\right)\\right)^2 \\right]$$\n",
    "\n",
    "The actor network is updated in an alternating fashion to maximizes the expected Q-value.\n",
    "\n",
    "<!-- ![](figures/actor_formula.png) -->\n",
    "\n",
    "$$J_p(\\pi_\\phi) := \\mathbb{E}_{s\\sim D, a\\sim\\pi_\\phi(\\cdot|s)} \\left[ Q_\\phi(s,a) \\right]$$\n",
    "\n",
    "### Conservative Q-Learning\n",
    "\n",
    "As of 2021, Conservative Q-Learning [@Kumar2020ConservativeQF] is the state-of-the-art for offline RL. It uses a “simple Q-value regularizer” to prevent the overestimation of OOD actions. \n",
    "\n",
    "<!-- ![](figures/cql.png) -->\n",
    "\n",
    "$$\\min_\\phi J_q(Q_\\phi)+\\alpha(\\mathbb{E}_{s\\sim D, a\\sim\\mu(\\cdot|s)}[Q_\\phi(s,a)] - \\mathbb{E}_{(s,a)\\sim D}[Q_\\phi(s,a)])$$\n",
    "\n",
    "<!-- TODO: check if it's really \\mu and not \\pi_\\phi -->\n",
    "\n",
    "For each state, CQL computes a distribution over actions using a temperature parameter $\\alpha$ that controls the amount of exploration. The distribution is a mixture of the behavior policy and the current Q-function. The closer $\\alpha$ is to 1 the more conservative.\n",
    "\n",
    "CQL will be used as the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c651ef-5fc8-4d69-83f4-3d9edb5f0a6b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3b9aa",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic (SAC-N)\n",
    "\n",
    "![Structure of deep actor-critic RL with multiple critics, as in SAC-N^[Figure taken from @app10124236 (modified)]](figures/actor_critic_sacn.png){fig-align=\"center\" #fig-actor-critic-sacn width=80%}\n",
    "\n",
    "The paper introduces two new methods for offline RL. The first method is called SAC-N and is an extension of Soft Actor-Critic (SAC) [@Haarnoja2018SoftAO], which is a popular off-policy actor-critic deep RL algorithm.\n",
    "SAC-N extends SAC by using the q-value of N instead of two q-functions, i.e. critics, as visualized in #fig-actor-critic-sacn. The q-values are then reduces to a single value by taking the minimum. The idea behind taking the minimum of more critics is that the resulting q-value is more pessimistic when the uncertainty is high. This prevents erroneously high q-values of OOD actions and therefore trains the actor to prefer safer actions.\n",
    "\n",
    "The minimum of multiple critics approximates the true q-value minus a multiple of the standard deviation [@an2021edac]:\n",
    "\n",
    "$$ \\mathbb{E}\\left [\\min_{i=1,...,N}Q_i\\right] \\approx m - \\Phi^{-1}\\left(\\frac{N-\\pi/8}{N-\\pi/4+1}\\right) \\sigma $$\n",
    "\n",
    "Where $N$ is the number of critics, $Q_i$ is the q-value of the $i$-th critic, $m$ is the theoretical true q-value, $\\Phi$ is the CDF of the standard gaussian distribution, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "This is visualized in the diagram below, where q-value estimates over an exemplary action space are plotted.The black line is the theoretical true q-value and the grey area its standard deviation. The lightblue lines represent the critics, that try to approximate the true q-value. The bottom blue line is the minimum of the critics, that should, especially for a high number of critics, be roughly the true q-value minus a multiple of the standard deviation. You can use the slider to change the number of critics:\n",
    "\n",
    "```{ojs #fig-critics-slider}\n",
    "means = [0,2,-1,0,1,1,2,0,-1,-2,0,1,2,4,3,3,1,0,-1,0];\n",
    "stds  = [2,3, 2,1,2,3,2,3, 4, 2,3,4,2,1,3,2,1,2, 3,2];\n",
    "viewof num_critics = Inputs.range([1,50], {value:20, step:1, label: \"#critics: \"});\n",
    "function gaussianRandom(mean=0, stdev=1) {\n",
    "    let u = 1 - Math.random(); // Converting [0,1) to (0,1]\n",
    "    let v = Math.random();\n",
    "    let z = Math.sqrt(-2.0*Math.log(u)) * Math.cos(2.0*Math.PI*v);\n",
    "    return z * stdev + mean;\n",
    "}\n",
    "function example_critic() {\n",
    "  return [...Array(means.length).keys()].map((i)=>gaussianRandom(means[i], stds[i]));\n",
    "}\n",
    "function toPlot(data, color){\n",
    "  return Plot.line(data.map((x, i)=>({\"action space\": i, \"q-value\": x})), {x: \"action space\", y: \"q-value\", stroke: color});\n",
    "}\n",
    "critics = [...Array(num_critics).keys()].map(i=>example_critic());\n",
    "Plot.plot({\n",
    "  marks: [\n",
    "      Plot.areaY(means.map((x,i)=>({\"action space\": i, low:x-stds[i], high:x+stds[i]})), {x: \"action space\", y1: \"low\", y2: \"high\", fill: \"#ddd\"}),\n",
    "      ...(critics.map(x=>toPlot(x, \"lightblue\"))),\n",
    "      toPlot([...Array(means.length).keys()].map(i=>Math.min(...critics.map(x=>x[i]))), \"blue\"),\n",
    "      toPlot(means, \"black\"),\n",
    "  ],\n",
    "  y: {\n",
    "    domain: [-10,10],\n",
    "    label: \"q-value\",\n",
    "  },\n",
    "  x: {\n",
    "    tickFormat: x => \"\",\n",
    "  },\n",
    "  width: 796,\n",
    "});\n",
    "```\n",
    "\n",
    "SAC-N already achieves notable performance and beats the previous state of the art, CQL, as will be shown in the [results section](#results). However, SAC-N requires a large number of critics, which comes with a high computational cost. Therefore, the paper introduces a second method, EDAC, that is more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682613d3",
   "metadata": {},
   "source": [
    "## Ensemble-Diversified Actor Critic (EDAC)\n",
    "\n",
    "@an2021edac found, that the performance of the policy learned by SAC-N decreases significantly, when the q-functions share a similar local structure. To reduce this, they introduce an ensemble gradient diversification term to the loss function of the ensemble of critics:\n",
    "\n",
    "$$ \\underset\\phi{\\text{minimize}}\\ \\ \\frac{1}{N-1} \\sum_{1\\leq i\\neq j \\leq N} \\langle \\nabla_a Q_{\\phi_i}, \\nabla_a Q_{\\phi_j} \\rangle $$\n",
    "\n",
    "It measures the cosine similarity between the q-function gradients and is minimized when the gradients for the critics are as different as possible. This, in turn, leads to a more diverse ensemble of critics, which is more robust against overestimation of OOD actions.\n",
    "\n",
    "![Illustration of the ensemble gradient diversification. The vector $\\lambda_iw_i$ represents the normalized eigenvector $w_i$ of $\\text{Var}(\\nabla_a Q_{\\phi_j}(s,a))$ multiplied by its eigenvalue $\\lambda_i$. ^[Figure taken from @an2021edac]](figures/EDAC_gradient_diversification.png){#fig-EDAC}\n",
    "\n",
    "The full loss function of the critics is then:\n",
    "\n",
    "\n",
    "$$\\nabla_{\\phi_i} \\frac{1}{|B|} \\sum_{(s,a,r,s')\\in B} \\left (\\left( Q_{\\phi_i}(s,a) - y(r, s') \\right) + \\frac{\\eta}{N-1} \\sum_{1\\leq i\\neq j \\leq N} \\langle \\nabla_a Q_{\\phi_i}, \\nabla_a Q_{\\phi_j} \\rangle \\right)$$\n",
    "\n",
    "where $B$ is the batch of transitions, $y(r, s')$ is the target q-function^[TODO: maybe describe what the target q-function is], and $\\eta$ is the hyperparameter for how much the ensemble gradient diversification term should be weighted.\n",
    "\n",
    "Note that EDAC reduces to SAC-N when $\\eta=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ff798",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "There are multiple implementations of EDAC and SAC-N available.\n",
    "@an2021edac published their implementation on [GitHub](https://github.com/snu-mllab/EDAC). It contains 9712 lines of python code over 93 files.\n",
    "\n",
    "Another implementation is part of the Clean Offline Reinforcement Learning (CORL) [Repository](https://github.com/tinkoff-ai/CORL), which aims to provide single-file implementations of SOTA offline RL algorithms. Its EDAC implementation contains 639 lines of code in a single file. This makes it significantly easier to understand and modify. We therefore used it for some of our experiments and as a inspiration for our own implementation.^[Our fork of CORL for our experiments is also available on [GitHub](https://github.com/JonasLoos/CORL)]\n",
    "\n",
    "We also [implemented EDAC](https://github.com/JonasLoos/edac_reimplementation) from scratch and managed to achieve a code size of 379 lines, while adding additional features^[We added features like the continuation of training runs, and new critic ensemble reduction functions (instead of min) to our implementation.]. Our [results](#results) below are based on our implementation until stated otherwise.\n",
    "\n",
    "### Code\n",
    "\n",
    "A simplified version of the main parts of our [`train`](https://github.com/JonasLoos/edac_reimplementation/blob/main/edac.py#L152) function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1650742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-fold: true\n",
    "#| code-overflow: wrap\n",
    "#| code-summary: \"`def train(config, ...):`\"\n",
    "# initialize environment, and set seeds\n",
    "...\n",
    "\n",
    "# initialize models\n",
    "actor = Actor([state_dim, 256, 256, action_dim], ...)\n",
    "critic = VectorCritic([state_dim + action_dim, 256, 256, 1], ...)\n",
    "target_critic = deepcopy(critic)\n",
    "log_beta = torch.tensor(0.0, requires_grad=True)\n",
    "beta = log_beta.exp()\n",
    "\n",
    "# initialize optimizers (Adam)\n",
    "...\n",
    "\n",
    "# set critic ensemble reduction function, by default `min`\n",
    "...\n",
    "\n",
    "# load checkpoint if given, save the config, and initialize logging\n",
    "...\n",
    "\n",
    "# main training loop\n",
    "for epoch in range(config.epochs):\n",
    "\n",
    "    for step in range(config.updates_per_epoch):\n",
    "        # sample batch of transitions\n",
    "        state, action, reward, next_state, done = buffer.sample()\n",
    "\n",
    "        # calculate q-target\n",
    "        next_action, log_prob = actor(next_state)\n",
    "        q_next = (\n",
    "            critic_reduction(target_critic(next_state, next_action))\n",
    "             - beta * log_prob\n",
    "        )\n",
    "        q_target = reward + config.gamma * (1 - done) * q_next\n",
    "\n",
    "        # update critics\n",
    "        base_critic_loss = (critic(state, action) - q_target).pow(2)\n",
    "        q_gradients = torch.autograd.grad(critic(...), ..., create_graph=True)\n",
    "        diversity_loss = (q_gradients @ q_gradients.T) * (1-torch.eye(N)) / (N-1)\n",
    "        critic_loss = base_critic_loss.sum(-1) + config.eta * diversity_loss.sum(1,2)\n",
    "        ...\n",
    "\n",
    "        # update beta\n",
    "        beta_loss = (-log_beta * (actor_action_log_prob - action_dim))\n",
    "        ...\n",
    "        beta = log_beta.exp()\n",
    "\n",
    "        # update actor\n",
    "        actor_q_values = critic(state, actor_action)\n",
    "        actor_loss = -(critic_reduction(actor_q_values) - beta * actor_action_log_prob)\n",
    "        ...\n",
    "\n",
    "        # update target critic\n",
    "        for target_param, source_param in zip(\n",
    "                target_critic.parameters(), critic.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                (1 - config.tau) * target_param.data\n",
    "                 + config.tau * source_param.data\n",
    "            )\n",
    "\n",
    "    # save checkpoint, and log metrics\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3fa7d",
   "metadata": {},
   "source": [
    "Which uses separate classes for the `Actor` and `VectorCritic`^[The `VectorCritic` is a wrapper around a list of critics, which simplifies the handling of multiple critics.]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-fold: true\n",
    "#| code-overflow: wrap\n",
    "#| code-summary: \"`class Actor(nn.Module):`\"\n",
    "def __init__(self, layer_sizes : list[int], ...):\n",
    "    ...\n",
    "    # setup hidden layers based on the given layer sizes\n",
    "    self.hidden = nn.Sequential(*(\n",
    "        x for i in range(len(layer_sizes) - 2) for x in [\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i + 1]),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "    ))\n",
    "    # create output and output uncertainty layers\n",
    "    self.output = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "    self.output_uncertainty = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "    # init parameters as in the EDAC paper\n",
    "    ...\n",
    "\n",
    "def forward(self, state):\n",
    "    x_hidden = self.hidden(state)\n",
    "    x_mean = self.output(x_hidden)\n",
    "    x_std = torch.exp(torch.clip(self.output_uncertainty(x_hidden), -5, 2))\n",
    "    policy_dist = Normal(x_mean, x_std)\n",
    "    action_linear = policy_dist.rsample()\n",
    "    action = torch.tanh(action_linear) * self.max_action\n",
    "    action_log_prob = policy_dist.log_prob(action_linear).sum(-1)\n",
    "    return action, action_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45182bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| echo: true\n",
    "#| code-fold: true\n",
    "#| code-overflow: wrap\n",
    "#| code-summary: \"`VectorCritic(nn.Module):`\"\n",
    "def __init__(self, layer_sizes: list[int], num_critics: int):\n",
    "    ...\n",
    "    # create multiple critics with the architecture given by layer_sizes\n",
    "    # the output layer has no activation function\n",
    "    self.models = nn.ModuleList([\n",
    "        nn.Sequential(*[\n",
    "            x for i in range(len(layer_sizes) - 1) for x in [\n",
    "                nn.Linear(layer_sizes[i], layer_sizes[i + 1]),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "        ][:-1]) for _ in range(num_critics)\n",
    "    ])\n",
    "    # init parameters as in the EDAC paper\n",
    "    ...\n",
    "\n",
    "def forward(self, state, action):\n",
    "    return torch.cat([\n",
    "        model(torch.cat([state, action], dim=-1)) for model in self.models\n",
    "    ], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b69596",
   "metadata": {},
   "source": [
    "### Modifications\n",
    "\n",
    "We made a few modifications to the EDAC algorithm described in the paper.\n",
    "\n",
    "#### Dynamic $\\beta$\n",
    "\n",
    "As can be seen in the `train` function above, $\\beta$ (`beta`) is learned dynamically during training. While this is not explicitly mentioned in the pseudocode in the paper, the official implementation uses it, if `use_automatic_entropy_tuning` is set to `True`. As it can significantly improve training speed and the performance of the actor (at least for a limited training time), we decided to use it as well.\n",
    "\n",
    "However, we found that sometimes $\\beta$ would decrease continuously, which also limits the performance of the actor. How to prevent this could be a topic for future research.\n",
    "\n",
    "#### Alternative Critic Ensemble Reduction\n",
    "\n",
    "SAC-N, and therefore also EDAC, use the minimum q-value estimate of their critic ensembles. However, it seems like this could be too pessimistic in some cases, as q-value estimates cannot only be erroneously high, but also erroneously low. We tried to get better q-value estimations by subtracting a multiple of the standard deviation $\\sigma$ from the mean $\\mu$, i.e. $Q_{\\text{final}} = \\mu - \\alpha * \\sigma$, where $\\alpha$ is a hyperparameter. The idea is to be more pessimistic, the higher the standard deviation is. Actually, as already mentioned in the section about [SAC-N](#soft-actor-critic-sac-n), this should be the expected value of the minimum q-value estimate for some specific $\\alpha$ and therefore be similar. However, as can be seen below in @fig-critic_reduction, it performs significantly worse than the minimum.\n",
    "\n",
    "Alternative critic ensemble reduction functions could be based on the median or use a weighted average of the q-values, where the weights decay exponentially for higher q-values. This, too, could be a topic for future research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3bee1",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "::: {#fig-vid-halfcheetah}\n",
    "{{< video figures/EDAC_reimplementation_halfcheetah.mp4 >}}\n",
    "This is our edac reimplementation on the halfcheetah task.\n",
    ":::\n",
    "\n",
    "::: {#fig-vid-walker2d}\n",
    "{{< video figures/EDAC_reimplementation_walker2d-full-replay-230319-164301.mp4 width=\"500px\" >}}\n",
    "This is our edac reimplementation on the walker2d task.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504ca5a-4c70-4c92-b204-3c3d289778b1",
   "metadata": {},
   "source": [
    "![](figures/batch_learning_time.png)\n",
    "\n",
    "Learning curve is similar for all batch sizes. With n=2048 we have optimal performance.\n",
    "Note: In the paper they had batch size n=256 although they used the same graphic card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "# imports for plotting the results\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbccdcf-bfaf-4c10-b10d-f1c99765a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-datasets\n",
    "#| fig-cap: \"Different Training Datasets\"\n",
    "#| echo: false\n",
    "\n",
    "df2 = pd.read_csv('data/datasets2.csv', delimiter=\";\")\n",
    "df2 = df2[0:400]\n",
    "\n",
    "df2.columns = ['epoch', 'full-replay', 'expert', 'medium-v2']\n",
    "\n",
    "x=df2['epoch']\n",
    "y=df2[['full-replay', 'expert', 'medium-v2']]\n",
    "labels = y.columns\n",
    "# colors = px.colors.sequential.Spectral\n",
    "#Plotly3, Plasma, thermal, Turbo\n",
    "# colors = random.sample(px.colors.diverging.PiYG, k=6)\n",
    "#Spectral, Picnic, RdYlGn, PiYG\n",
    "colors = px.colors.cyclical.HSV[::-1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(0, 3):\n",
    "    fig.add_trace(go.Scatter(x=x, y=y.iloc[:,i], mode='lines',\n",
    "        name=labels[i],\n",
    "        line=dict(color=colors[i], width=1.5)\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Full-Replay best dataset, Expert doesn\\'t train',\n",
    "    xaxis=dict(\n",
    "        title=\"Epoch\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        ),\n",
    "    ),\n",
    "        yaxis=dict(\n",
    "        title=\"Average Return\",\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        )\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=100,\n",
    "        r=20,\n",
    "        t=110,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Dataset\",\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2c8f0-9149-4a8e-acdc-b83becc88f5f",
   "metadata": {},
   "source": [
    "Interpretation: Our implementation performs best on the full-replay dataset. From mistakes made during training the model can learn. The full replay has the best mix between these errors and good actions. This way there are less OOD actions.\n",
    "Problem: The model should be able to learn from biased data, especially from expert data. It is possible though, that training with the expert dataset just needs much more time to kickstart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5f7f6-e04d-4dd4-a45a-4e2add20e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-num_critics\n",
    "#| fig-cap: \"Number of Critics\"\n",
    "#| echo: false\n",
    "\n",
    "df3 = pd.read_csv('data/num_critics.csv', delimiter=\";\")\n",
    "df3 = df3[0:400]\n",
    "\n",
    "df3.columns = ['epoch', 'EDAC-10', 'EDAC-50', 'EDAC-20', 'EDAC-2', 'EDAC-5', 'SAC-20', 'SAC-10']\n",
    "\n",
    "x=df3['epoch']\n",
    "y=df3[['SAC-10','SAC-20','EDAC-2', 'EDAC-5', 'EDAC-10', 'EDAC-20', 'EDAC-50']]\n",
    "labels = y.columns\n",
    "# colors = px.colors.sequential.Spectral\n",
    "#Plotly3, Plasma, thermal, Turbo\n",
    "# colors = random.sample(px.colors.diverging.PiYG, k=6)\n",
    "#Spectral, Picnic, RdYlGn, PiYG\n",
    "colors = px.colors.cyclical.HSV[::-1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(0, 7):\n",
    "    fig.add_trace(go.Scatter(x=x, y=y.iloc[:,i], mode='lines',\n",
    "        name=labels[i],\n",
    "        line=dict(color=colors[i], width=1.5)\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title='EDAC-10 outperforms SAC-20',\n",
    "    xaxis=dict(\n",
    "        title=\"Epoch\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        ),\n",
    "    ),\n",
    "        yaxis=dict(\n",
    "        title=\"Average Return\",\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        )\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=100,\n",
    "        r=20,\n",
    "        t=110,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Dataset\",\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ade09-4649-48d6-a1a2-5c64c437865c",
   "metadata": {},
   "source": [
    "Interpretation: EDAC with only 10 critics can outperform SAC-20. Increasing N did not always improve the results for EDAC.\n",
    "The training duration is heavily dependant on the amount of critics.\n",
    "\n",
    "![Median time per epoch in seconds on the halfcheetah-full-replay dataset. This includes the training time with 500 steps and a batch size of 2048, and the testing time with 5 evaluation episodes. The experiments were run on a GeForce RTX 3090.](figures/time_per_epoch.svg){#fig-epoch-time fig-align=\"center\" width=80%}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc24d9-e847-4ff3-bd24-ed08d3227c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| label: fig-critic_reduction\n",
    "#| fig-cap: \"TODO: Critic Reduction figure caption\"\n",
    "#| echo: false\n",
    "\n",
    "df = pd.read_csv('data/critic_reduction.csv', delimiter=\",\")\n",
    "\n",
    "df.columns = ['epoch', 'mean-8', 'mean-4', 'mean-2', 'mean-1', 'mean-0.5', 'mean', 'minimum']\n",
    "\n",
    "\n",
    "x=df['epoch']\n",
    "y=df[['mean-4', 'mean-2', 'mean-1', 'mean-0.5', 'mean', 'minimum']]\n",
    "labels = y.columns\n",
    "# colors = px.colors.sequential.Spectral\n",
    "#Plotly3, Plasma, thermal, Turbo\n",
    "# colors = random.sample(px.colors.diverging.PiYG, k=6)\n",
    "#Spectral, Picnic, RdYlGn, PiYG\n",
    "colors = px.colors.cyclical.HSV[::-1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(0, 6):\n",
    "    fig.add_trace(go.Scatter(x=x, y=y.iloc[:,i], mode='lines',\n",
    "        name=labels[i],\n",
    "        line=dict(color=colors[i], width=1.5)\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Custom Critic Reduction worse than Minimum',\n",
    "    xaxis=dict(\n",
    "        title=\"Epoch\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        ),\n",
    "    ),\n",
    "        yaxis=dict(\n",
    "        title=\"Average Return\",\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        )\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=100,\n",
    "        r=20,\n",
    "        t=110,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Model\",\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056215a-8038-47c0-b884-2cc7b9e13340",
   "metadata": {},
   "source": [
    "Interpretation: Using a custom critic reduction instead of the minimum, did not work with EDAC. The standard deviation of the ensemble might fluctuate too much to be \"contained\" by a static parameter.\n",
    "Other approaches could be to try out this method with SAC-N. Another idea is to take the median instead of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9eb25-06b1-4314-9e76-7141d7f3b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-test\n",
    "#| fig-cap: \"fig-cap example2\"\n",
    "#| echo: false\n",
    "#| eval: false  # do not evaluate, as this is a test only\n",
    "\n",
    "# TODO: delete this test cell\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Build figure\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"Step\"], y=df[\"Group: EDAC-D4RL - eval/reward_mean\"], name='Line 1',\n",
    "                         line=dict(color='firebrick', width=2, dash='dash')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"Step\"], y=df[\"Group: EDAC-D4RL - eval/reward_mean__MIN\"], name='Line 2',\n",
    "                         mode='lines+markers', line=dict(color='royalblue', width=2,)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"Step\"], y=df[\"Group: EDAC-D4RL - eval/reward_mean__MAX\"], name='Line 3',\n",
    "                         line=dict(color='goldenrod', width=2)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Test Graph',\n",
    "    xaxis=dict(\n",
    "        title=\"Steps\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        ),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Average Return\",\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        )\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=100,\n",
    "        r=20,\n",
    "        t=110,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Legend Title\",\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6899e83-a715-49b0-90df-ec7101437146",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4e06d2-b573-453b-8dfa-9ad91271eef4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
