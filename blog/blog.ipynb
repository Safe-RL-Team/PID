{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2ce5283f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\"\n",
    "description: \"based on [@an2021edac]\"\n",
    "author:\n",
    "    - name: Julian Dralle\n",
    "      affiliation: TU Berlin\n",
    "    - name: Jonas Loos\n",
    "      affiliation: TU Berlin\n",
    "bibliography: references.bib\n",
    "abstract: |\n",
    "    TODO: abstract\n",
    "jupyter: python3\n",
    "execute:\n",
    "  enabled: true\n",
    "  echo: false\n",
    "  output: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f2ad2-0d2e-4e23-97d7-1874247f2886",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Why Offline-RL?\n",
    "\n",
    "Training of RL algorithms require **active interaction** with the environment. Training can become quite time-consuming and expensive. It can even be dangerous in safety-critical domains like driving or healthcare. A trial-and-error procedure is basically prohibited. We cannot as an example let an agent explore, make mistakes, and learn while treating patients in a hospital.\n",
    "That's what makes learning from pre-collected experience so relevant. And fortunately we have already in many domains existing large datasets.\n",
    "Offline-RL therefore aims to learn policies using only these pre-collected data without further interactions with the environment.\n",
    "\n",
    "![Online and Offline Reinforcement Learning](figures/offline_rl.gif){fig-align=\"center\" width=80%}\n",
    "\n",
    "### What properties make offline-RL difficult?\n",
    "\n",
    "But offline RL comes with its own challenges. By far the biggest problem are so called **out of distribution (OOD) actions**. OOD actions refer to actions taken by an agent that fall outside the range of actions observed in the training dataset. \n",
    "State-action space can become so vast that the dataset cannot cover all of it. Especially narrow and biased datasets lack significant coverage and can lead to problems with OOD actions. For example, healthcare datasets are often biased towards serious cases. Only seriously ill people are getting treated, while healthier people are sent home untreated.\n",
    "\n",
    "![](figures/ood_medicine.png){fig-align=\"center\" width=80%}\n",
    "\n",
    "A naive algorithm might now conclude that treatment causes death, since there were no fatalities in the untreated (= healthy) patients. Choosing to not treat a severely sick patient is something that never happened in the data, since the doctor would thereby violate his duty of care. Not treating a sick patient is therefore an OOD action. \n",
    "Vanilla RL algorithm might heavily overestimate the Q-values of OOD state-action pairs.\n",
    "\n",
    "### How to deal with OOD state-actions?\n",
    "\n",
    "\"Avoid OOD state-actions!\", has been the approach of many offline RL algorithms. This can be achieved by regularizing the policy to be close to the behavior policy that was used to collect the data. A more recent approach is to penalize the Q-values to be more pessimistic as done in Conservative Q-learning for Offline RL (CQL).\n",
    "But if we use this approach we require either (a) an estimation of the behavior policy or (b) explicit sampling from OOD data points (difficult!). Further, we prohibit our agent to approach any OOD state-actions, while some of these might actually be good. Q-function networks **do** have the ability to generalize. It's all about **handling the uncertainty** of these predictions. The agent might benefit from choosing some OOD data points which Q-values we can predict with high confidence.\n",
    "With SAC-N and EDAC An et al. (2021) found a way of **effectively quantifying the Q-value estimates** by an ensemble of Q-function networks. In this blog we will explore and explain them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25d15ad6-be20-4cdc-bea7-c22023923d4c",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "\n",
    "### Q-Learning\n",
    "Like in standard reinforcement learning we want to find a policy $\\pi(a | s)$ that maximizes the cumulative discounted reward $\\mathbb{E}_{s_t, a_t}[...]$. \n",
    "The model-free [Q-learning](https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c) algorithm is a very common approach to learn the Q-function $Q_{\\phi}(s,a)$ with-in a neural network.\n",
    "\n",
    "### Actor-critic method\n",
    "In the standard deep actor-critic approach we use two networks: (1) a policy-based actor network and (2) a value-based critic network.\n",
    "\n",
    "![Structure of deep actor-critic RL](https://www.mdpi.com/applsci/applsci-10-04236/article_deploy/html/images/applsci-10-04236-g005.png){fig-align=\"center\" width=80%}\n",
    "\n",
    "The critic network minimizes the Bellman residual. Note: In offline RL transitions are sampled from a static dataset $D$\n",
    "\n",
    "<!-- ![](figures/critic_formula.png) -->\n",
    "\n",
    "$$J_q(Q_\\phi) := \\mathbb{E}_{(s,a,s') \\sim D} \\left[ \\left( Q_\\phi(s,a) - \\left ( r(s,a) + \\gamma\\ \\mathbb{E}_{a'\\sim\\pi_\\phi(\\cdot|s')}[Q_{\\phi'}(s',a')] \\right)\\right)^2 \\right]$$\n",
    "\n",
    "The actor network is updated in an alternating fashion to maximizes the expected Q-value.\n",
    "\n",
    "<!-- ![](figures/actor_formula.png) -->\n",
    "\n",
    "$$J_p(\\pi_\\phi) := \\mathbb{E}_{s\\sim D, a\\sim\\pi_\\phi(\\cdot|s)} \\left[ Q_\\phi(s,a) \\right]$$\n",
    "\n",
    "### Conservative Q-Learning\n",
    "\n",
    "As of 2021, Conservative Q-Learning [@Kumar2020ConservativeQF] is the state-of-the-art for offline RL. It uses a “simple Q-value regularizer” to prevent the overestimation of OOD actions. \n",
    "\n",
    "<!-- ![](figures/cql.png) -->\n",
    "\n",
    "$$\\min_\\phi J_q(Q_\\phi)+\\alpha(\\mathbb{E}_{s\\sim D, a\\sim\\mu(\\cdot|s)}[Q_\\phi(s,a)] - \\mathbb{E}_{(s,a)\\sim D}[Q_\\phi(s,a)])$$\n",
    "\n",
    "<!-- TODO: check if it's really \\mu and not \\pi_\\phi -->\n",
    "\n",
    "For each state, CQL computes a distribution over actions using a temperature parameter $\\alpha$ that controls the amount of exploration. The distribution is a mixture of the behavior policy and the current Q-function. The closer $\\alpha$ is to 1 the more conservative.\n",
    "\n",
    "CQL will be used as the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c651ef-5fc8-4d69-83f4-3d9edb5f0a6b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43b3b9aa",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic (SAC-N)\n",
    "\n",
    "The paper introduces two new methods for offline RL. The first method is called SAC-N and is an extension of Soft Actor-Critic (SAC) [@Haarnoja2018SoftAO], which is a popular off-policy actor-critic deep RL algorithm.\n",
    "SAC-N extends SAC by using the minimum q-value of N instead of two q-functions, i.e. critics. The idea behind using the minimum of more critics is that the resulting q-value is more pessimistic when the uncertainty is high. This prevents erroneously high q-values of OOD actions and therefore trains the actor to prefer safer actions.\n",
    "\n",
    "The minimum of multiple critics approximates the true q-value minus a multiple of the standard deviation [@an2021edac]:\n",
    "\n",
    "$$ \\mathbb{E}\\left [\\min_{i=1,...,N}Q_i\\right] \\approx m - \\Phi^{-1}\\left(\\frac{N-\\pi/8}{N-\\pi/4+1}\\right) \\sigma $$\n",
    "\n",
    "Where $N$ is the number of critics, $Q_i$ is the q-value of the $i$-th critic, $m$ is the theoretical true q-value, $\\Phi$ is the CDF of the standard gaussian distribution, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "This is visualized in the diagram below, where q-value estimates over an exemplary action space are plotted.The black line is the theoretical true q-value and the grey area its standard deviation. The lightblue lines represent the critics, that try to approximate the true q-value. The bottom blue line is the minimum of the critics, that should, especially for a high number of critics, be roughly the true q-value minus a multiple of the standard deviation. You can use the slider to change the number of critics:\n",
    "\n",
    "```{ojs}\n",
    "means = [0,2,-1,0,1,1,2,0,-1,-2,0,1,2,4,3,3,1,0,-1,0];\n",
    "stds  = [2,3, 2,1,2,3,2,3, 4, 2,3,4,2,1,3,2,1,2, 3,2];\n",
    "viewof num_critics = Inputs.range([1,50], {value:20, step:1, label: \"#critics: \"});\n",
    "function gaussianRandom(mean=0, stdev=1) {\n",
    "    let u = 1 - Math.random(); // Converting [0,1) to (0,1]\n",
    "    let v = Math.random();\n",
    "    let z = Math.sqrt(-2.0*Math.log(u)) * Math.cos(2.0*Math.PI*v);\n",
    "    return z * stdev + mean;\n",
    "}\n",
    "function example_critic() {\n",
    "  return [...Array(means.length).keys()].map((i)=>gaussianRandom(means[i], stds[i]));\n",
    "}\n",
    "function toPlot(data, color){\n",
    "  return Plot.line(data.map((x, i)=>({\"action space\": i, \"q-value\": x})), {x: \"action space\", y: \"q-value\", stroke: color});\n",
    "}\n",
    "critics = [...Array(num_critics).keys()].map(i=>example_critic());\n",
    "Plot.plot({\n",
    "  marks: [\n",
    "      Plot.areaY(means.map((x,i)=>({\"action space\": i, low:x-stds[i], high:x+stds[i]})), {x: \"action space\", y1: \"low\", y2: \"high\", fill: \"#ddd\"}),\n",
    "      ...(critics.map(x=>toPlot(x, \"lightblue\"))),\n",
    "      toPlot([...Array(means.length).keys()].map(i=>Math.min(...critics.map(x=>x[i]))), \"blue\"),\n",
    "      toPlot(means, \"black\"),\n",
    "  ],\n",
    "  y: {\n",
    "    domain: [-10,10],\n",
    "    label: \"q-value\",\n",
    "  },\n",
    "  x: {\n",
    "    tickFormat: x => \"\",\n",
    "  }\n",
    "});\n",
    "```\n",
    "\n",
    "SAC-N already achieves notable performance and beats the previous state of the art, CQL, as will be shown in [Results](#results). However, SAC-N requires a large number of critics, which comes with a high computational cost. Therefore, the paper introduces a second method, EDAC, that is more efficient.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "682613d3",
   "metadata": {},
   "source": [
    "## Ensemble-Diversified Actor Critic (EDAC)\n",
    "\n",
    "EDAC improves over SAC-N, by introducing an ensemble gradient diversification term to the optimization of the ensemble of critics:\n",
    "\n",
    "$$ \\underset\\phi{\\text{minimize}}\\ \\ \\frac{1}{N-1} \\sum_{1\\leq i\\neq j \\leq N} \\langle \\nabla_a Q_{\\phi_i}, \\nabla_a Q_{\\phi_j} \\rangle $$\n",
    "\n",
    "This term is added to the loss function of the critics and is minimized when the gradients for the critics are as different as possible. This, in turn, leads to a more diverse ensemble of critics, which is more robust against overestimation of OOD actions.\n",
    "\n",
    "![Illustration of the ensemble gradient diversification. The vector $\\lambda_iw_i$ represents the normalized eigenvector $w_i$ of $\\text{Var}(\\nabla_a Q_{\\phi_j}(s,a))$ multiplied by its eigenvalue $\\lambda_i$.](figures/EDAC_gradient_diversification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3bee1",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "{{< video figures/EDAC_reimplementation_halfcheetah.mp4 >}}\n",
    "This is a our edac reimplementation on the halfcheetah task.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b7f3f-893e-4f71-87e0-16451bb5943a",
   "metadata": {},
   "source": [
    "For a demonstration of a simple line-plot, see @test-label1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cbfe0-b29a-4e41-96b9-298a58c90168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: test-label1\n",
    "#| fig-cap: \"fig-cap example1\"\n",
    "#| echo: false\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "\n",
    "fig = px.line(df, x=\"Step\", \n",
    "              y=[\"Group: EDAC-D4RL - eval/reward_mean\",\"Group: EDAC-D4RL - eval/reward_mean__MIN\",\"Group: EDAC-D4RL - eval/reward_mean__MAX\"],\n",
    "             title=\"Test Graph\",\n",
    "             markers=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9eb25-06b1-4314-9e76-7141d7f3b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: test-label2\n",
    "#| fig-cap: \"fig-cap example2\"\n",
    "#| echo: false\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Build figure\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"Step\"], y=df[\"Group: EDAC-D4RL - eval/reward_mean\"], name='Line 1',\n",
    "                         line=dict(color='firebrick', width=2, dash='dash')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"Step\"], y=df[\"Group: EDAC-D4RL - eval/reward_mean__MIN\"], name='Line 2',\n",
    "                         mode='lines+markers', line=dict(color='royalblue', width=2,)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"Step\"], y=df[\"Group: EDAC-D4RL - eval/reward_mean__MAX\"], name='Line 3',\n",
    "                         line=dict(color='goldenrod', width=2)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Test Graph',\n",
    "    xaxis=dict(\n",
    "        title=\"Steps\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        ),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Average Return\",\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Arial',\n",
    "            size=12,\n",
    "            color='rgb(82, 82, 82)',\n",
    "        )\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    margin=dict(\n",
    "        autoexpand=True,\n",
    "        l=100,\n",
    "        r=20,\n",
    "        t=110,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Legend Title\",\n",
    "    plot_bgcolor='white',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6899e83-a715-49b0-90df-ec7101437146",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4e06d2-b573-453b-8dfa-9ad91271eef4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
