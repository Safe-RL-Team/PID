<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Julian Dralle">
<meta name="author" content="Jonas Loos">
<meta name="description" content="based on (An et al. 2021)">

<title>Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="blog_files/libs/clipboard/clipboard.min.js"></script>
<script src="blog_files/libs/quarto-html/quarto.js"></script>
<script src="blog_files/libs/quarto-html/popper.min.js"></script>
<script src="blog_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="blog_files/libs/quarto-html/anchor.min.js"></script>
<link href="blog_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="blog_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="blog_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="blog_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="blog_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble</h1>
</div>

<div>
  <div class="description">
    based on <span class="citation" data-cites="an2021edac">(<a href="#ref-an2021edac" role="doc-biblioref">An et al. 2021</a>)</span>
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    Julian Dralle 
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    Jonas Loos 
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            TU Berlin
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    <p>TODO: abstract</p>
  </div>
</div>

</header>

<section id="filler-content" class="level1">
<h1>Filler Content</h1>
<blockquote class="blockquote">
<p>ChatGPT: write a blog article in markdown about “Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble”</p>
</blockquote>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems in a wide range of fields, including robotics, finance, and game AI. However, one major challenge in RL is how to effectively learn from limited data. Offline RL addresses this challenge by allowing RL agents to learn from previously collected data, rather than through trial-and-error interactions with the environment.</p>
<p>In this article, we will discuss a recent approach to offline RL called “Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble” (UOFRL-DQE). This approach addresses the problem of limited data by leveraging uncertainty estimates to guide the learning process and by creating a diverse ensemble of Q-functions to improve the robustness of the learned policy.</p>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In RL, an agent interacts with an environment and learns to take actions that maximize a cumulative reward signal. The agent’s decision-making process is guided by a Q-function, which estimates the expected reward for taking a given action in a given state. In offline RL, the agent is trained on a dataset of transitions (state, action, reward, next state) collected from past interactions with the environment.</p>
<p>One major challenge in offline RL is the problem of distributional shift, which occurs when the distribution of states and actions encountered during training is different from the distribution encountered during deployment. This can lead to poor performance of the learned policy.</p>
</section>
<section id="uofrl-dqe-approach" class="level2">
<h2 class="anchored" data-anchor-id="uofrl-dqe-approach">UOFRL-DQE Approach</h2>
<p>UOFRL-DQE addresses the problem of distributional shift by creating a diverse ensemble of Q-functions. Each Q-function in the ensemble is trained on a different subset of the training data, and the ensemble is used to estimate the Q-value of a given state-action pair.</p>
<p>In addition to creating a diverse ensemble, UOFRL-DQE also leverages uncertainty estimates to guide the learning process. The approach estimates the uncertainty of the Q-value estimates by measuring the variance of the Q-values across the ensemble. The agent then prioritizes learning from transitions with high uncertainty, in order to improve the overall performance of the learned policy.</p>
</section>
<section id="evaluation" class="level2">
<h2 class="anchored" data-anchor-id="evaluation">Evaluation</h2>
<p>The effectiveness of UOFRL-DQE has been evaluated in a variety of tasks, including robotic manipulation and game playing. The results have shown that UOFRL-DQE is able to effectively leverage uncertainty estimates and the diversity of the ensemble to improve the robustness of the learned policy, leading to better performance in comparison to other offline RL methods.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble (UOFRL-DQE) is a powerful approach to offline RL that addresses the problem of limited data by leveraging uncertainty estimates and creating a diverse ensemble of Q-functions. The approach has been shown to be effective in a variety of tasks and has the potential to significantly improve the performance of RL agents in real-world applications.</p>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-an2021edac" class="csl-entry" role="doc-biblioentry">
An, Gaon, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. 2021. <span>“Uncertainty-Based Offline Reinforcement Learning with Diversified q-Ensemble.”</span> In <em>Neural Information Processing Systems</em>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>